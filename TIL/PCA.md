# 얼굴인식에서 노이즈 제거를 위한 PCA 접근

PCA가 영상인식에 활용되는 대표적인 예는 얼굴인식(face recognition)입니다. <br/>
이와 관련된 개념 혹은 용어로서 eigenface(아이겐페이스)가 있습니다.
 
다음과 같은 20개의 45x40 얼굴 이미지들이 있습니다.
 
![image](https://user-images.githubusercontent.com/83413923/152647258-66609487-717c-48ba-a311-1dd3518d81b7.png)

> <그림 1> 45x40 얼굴 이미지 20장
 
이미지에서 픽셀 밝기값을 일렬로 연결하여 벡터로 만들면, 이들 각각의 얼굴 이미지는 45x40 = 1,800 차원의 벡터로 생각할 수 있습니다.<br/>
(즉, 각각의 이미지는 1,800 차원 공간에서 한 점(좌표)에 대응)
 
이제 이 20개의 1,800차원 점 데이터들을 가지고 PCA를 수행하면 데이터의 차원 수와 동일한 개수의 주성분 벡터들을 얻을 수 있습니다.<br/>
이렇게 얻어진 주성분 벡터들을 다시 이미지로 해석한 것이 eigenface 입니다. (얼굴 이미지를 가지고 얻은 벡터이기에 eigenface)

실제 위 이미지에 대해 얻어진 1,800개의 eigenface들 중 분산이 큰 순서대로  20개를 나열하면 아래 그림과 같습니다.

![image](https://user-images.githubusercontent.com/83413923/152647346-3406bd1d-b2b5-41bc-955b-ea85cdd15dba.png)
> <그림 2> 처음 20개의 eigenface들
 
위 그림에서 볼 수 있듯이 앞부분 eigenface들은 데이터들에 공통된 요소(얼굴의 전반적인 형태)를 나타내고 뒤로 갈수록 세부적인 차이 정보를 나타냅니다. 
그리고 더 뒤로 가면 거의 노이즈(noise)성 정보를 나타냅니다.
 
앞서 PCA를 통해 얻어진 주성분 벡터들은 서로 수직인 관계에 있다고 말한 바 있습니다. <br/>
이 말은 주성분 벡터들이 n차원 공간을 생성하는 기저(basis) 역할을 할 수 있음을 의미합니다. 
 
즉, PCA로 얻은 주성분 벡터들을 e1, e2, ..., en라면 임의의 n차원 데이터 x는 x = c1e1 + c2e2 + ... + cnen과 같이 ei들의 일차결합으로 표현될 수 있습니다.
 
그런데  뒷부분의 주성분 벡터들은 데이터 분포에 포함된 노이즈(noise)성 정보를 나타내기 때문에, 뒷부분은 버리고 전반부 k개의 주성분 벡터들만을 가지고 원래 데이터를 표현하면 노이즈가 제거된 데이터를 얻을 수 있습니다. 
 
즉, 원래의 x가 x = c1e1 + c2e2 + ... + cnen일 때 xk = c1e1 + ... +ckek로 x를 근사하는 것. <br/>
위 얼굴 이미지들에 대해 전반부의 일부(k = 20, 10, 5, 2) eigenface들만을 이용한 근사 이미지들은 아래 그림과 같습니다.

![image](https://user-images.githubusercontent.com/83413923/152647352-e1c2a9e5-9315-4db0-be2d-10a09f51a849.png)
> <그림 3> k개의 eigenface만을 이용한 데이터 복원(reconstruction)

그림에서 볼 수 있듯이 많은 수의 eigenface를 이용하면 원본 얼굴과 거의 유사한 근사(복원) 결과를 볼 수 있지만 k가 작아질수록 개인 고유의 얼굴 특성은 사라지고 공통의 얼굴 특성이 남게 됩니다.
(k=20인 경우 원래 얼굴이 그대로 살아나지만 k=2인 경우 개인간의 구분이 거의 사라짐을 볼 수 있습니다). <br/>
☞ 노이즈(noise)에 대해 좀 더 생각해 보면, 앞서 말했듯이 PCA는 개별 데이터에 대한 분석이 아니라 전체 데이터에 대한 집합적 분석 도구입니다. 
 
 만일 강아지 100마리에 대한 PCA 분석 결과가 있다고 합시다. 이 때, 강아지 데이터에서 얻어진 eigenface들 중 앞의 것(주성분의 개수가 적은 것)들은 강아지 공통의 형태 정보를 나타내고 뒤로 갈수록 강아지들 내부에서 강아지들 사이의 차이점을 표현할 수 있는 정보를 나타냅니다. 더 뒤로 나아가면 노이즈성 정보를 표현합니다. 
 
(어디서 어디까지가 데이터 공통 성분이고 어디까지가 데이터의 차이인지, 그리고 어디부터 노이즈 성분인지 그 구분은 명확하지 않습니다. 데이터에 따라서 주관적으로 또는 실험적으로 결정하는 것이 통상적입니다.) 
 
따라서, 위에서 설명한 k개의 주성분 벡터만을 이용하여 원래 데이터를 표현하는 것은 관점에 따라서 차원 감소(dimension reduction), 데이터 압축(compression), 노이즈 제거 등으로 다양하게 해석될 수 있습니다. <br/>
먼저, 차원감소라 함은 n차원의 데이터를 xk = c1e1 + ... + ckek로 표현했을 때 e1, ..., ek를 새로운 좌표축으로 하는 공간에서 x를 (c1, c2, ..., ck)와 같이 k차원의 점으로 표현한다는 의미입니다. <br/>
둘째, 데이터 압축의 의미는 {x}들을 그대로 저장하지 않고 k개의 주성분 벡터들과 계수 (c1, .., ck)들만을 저장하면 저장용량을 크게 줄일 수 있다는 의미입니다. <br/>
참고로 SVD(특이값분해)를 이용한 데이터 압축은 데이터를 개별적으로 압축하지만 PCA는 데이터를 집합적으로 압축한다는 점이 다릅니다. 

마지막으로, 노이즈 제거란 k개의 주성분만을 이용해서 데이터를 복원함으로써 의미없는 노이즈 부분을 날린다는 의미입니다.
